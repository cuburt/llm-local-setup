{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267b2307-b71a-457e-b782-a97a18a36bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a167dc-3f2f-4658-801a-1ee7232ca8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama_cpp_python\n",
      "Version: 0.2.28\n",
      "Summary: Python bindings for the llama.cpp library\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
      "License: MIT\n",
      "Location: /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages\n",
      "Requires: diskcache, numpy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show llama_cpp_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebefd09c-e51e-41ea-99d7-44deae5130ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.1/426.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b9757e-819c-4688-8a8e-14c859cd2b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1149  100  1149    0     0   2719      0 --:--:-- --:--:-- --:--:--  2722\n",
      "100 3891M  100 3891M    0     0  10.5M      0  0:06:09  0:06:09 --:--:-- 10.8M8k      0  0:06:58  0:00:26  0:06:32 10.3M   0  0:06:51  0:00:29  0:06:22 10.8M    0  9785k      0  0:06:47  0:00:33  0:06:14 10.3M06:25  0:01:02  0:05:23 10.8M6:19  0:01:33  0:04:46 10.8M    0  10.3M      0  0:06:17  0:01:51  0:04:26 10.8M10.3M      0  0:06:16  0:01:53  0:04:23 10.8M0  10.3M      0  0:06:15  0:02:07  0:04:08 10.3M10.4M      0  0:06:13  0:02:35  0:03:38 10.8M  0:06:13  0:02:44  0:03:29 10.8M   0  10.4M      0  0:06:13  0:02:52  0:03:21 10.8M     0  10.4M      0  0:06:12  0:02:56  0:03:16 10.8M    0  10.4M      0  0:06:13  0:02:58  0:03:15 10.3M86M    0     0  10.4M      0  0:06:12  0:03:00  0:03:12 10.3M.4M      0  0:06:12  0:03:14  0:02:58 10.8M 10.4M      0  0:06:12  0:03:21  0:02:51 10.3M    0  10.4M      0  0:06:11  0:03:30  0:02:41 10.3M 0     0  10.4M      0  0:06:11  0:03:40  0:02:31 10.3M     0  10.4M      0  0:06:11  0:03:49  0:02:22 10.3M 0  10.4M      0  0:06:11  0:03:52  0:02:19 10.3M     0  10.5M      0  0:06:10  0:04:22  0:01:48 10.3M    0  10.5M      0  0:06:10  0:04:42  0:01:28 10.3M     0  10.5M      0  0:06:10  0:04:49  0:01:21 10.9M  0     0  10.5M      0  0:06:09  0:05:00  0:01:09 10.8M 0  10.5M      0  0:06:09  0:05:33  0:00:36 10.3M0  10.5M      0  0:06:09  0:05:59  0:00:10 10.8M\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf --output ./models/llama-2-7b-chat.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f83bd6e-6f09-496b-b6f4-8b5ddb305eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d755aa-d5df-48f2-9cc5-18d56f3d26f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5d423-8d11-4dc6-9c64-71fa98eae2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60afdb-38f0-4acd-8664-99d3cb72a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     load_in_4bit=True,\n",
    "#     # config=model_config,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "#     use_auth_token=hf_auth,\n",
    "#     attention_sink_size=4,\n",
    "#     attention_sink_window_size=4092,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca781823-8b32-4626-8841-87509c753060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers\n",
      "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from ctransformers) (0.20.2)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: filelock in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2023.11.17)\n",
      "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-cpuinfo, ctransformers\n",
      "Successfully installed ctransformers-0.2.27 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09957b5b-ce41-4559-8057-de9558f72089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:690: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "hf_auth = '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'\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./models/llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=150, context_length=1024, hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True, use_auth_token=hf_auth, force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89029035-0e0e-4f91-8c72-9c0fc49f67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8307e8-e335-45d6-a9e9-b6477b6c0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a33928b-cafb-4b72-871d-05d4dfa1f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain bernoulli's equation and give examples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e5a106-5f0e-4889-b5bc-23ac46376250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e71b0a-7ef3-46e9-a9c9-e4ec02910c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain bernoulli's equation and give examples of its application.\n",
      "Bernoulli's equation is a mathematical relationship that relates the pressure and velocity of a fluid (liquid or gas) in motion. It is named after the Swiss mathematician and physicist Daniel Bernoulli, who first proposed the equation in the 18th century. The equation is given by:\n",
      "P + 1/2ρv^2 + ρg = constant\n",
      "where P is the pressure of the fluid, ρ is its density, v is its velocity, and g is the acceleration due to gravity (if applicable).\n",
      "The equation states that the sum of the pressure and half the density of the fluid's velocity squared plus the gravitational acceleration is constant along a streamline (a line in a fluid through which the fluid flows). In other words, the pressure of a fluid decreases as its velocity increases.\n",
      "Here are some examples of the application of Bernoulli's equation:\n",
      "1. Aircraft wings: The shape of an aircraft wing is designed to produce a pressure difference between the top and bottom surfaces of the wing, which generates lift. According to Bernoulli's equation, the pressure on the top surface of the wing is lower than the pressure on the bottom surface, causing the wing to lift off the ground.\n",
      "2. Hydrofoils: Hydrofoils are boats that use the shape of their hull to reduce drag and increase speed. By reducing the surface area of the hull that is exposed to the water, hydrofoils can reduce the drag on the boat and increase its speed. According to Bernoulli's equation, the pressure on the hull of a hydrofoil is lower than the pressure on the water around it, which reduces the drag on the boat.\n",
      "3. Wind turbines: Wind turbines use Bernoulli's equation to generate electricity. The blades of a wind turbine are designed to produce a pressure difference between the front and back of the blade, which causes the blade to rotate. According to Bernoulli's equation, the pressure on the front of the blade is lower than the pressure on the back, which generates lift and causes the blade to rotate.\n",
      "4. Pumps: Bernoulli's equation can be used to design pumps that can move fluids with minimal energy loss. By understanding the pressure and velocity of a fluid as it flows through a pump, engineers can design pumps that are more efficient and use less energy.\n",
      "5. Aircraft engines: Bernoulli's equation can be used to design aircraft engines that are more efficient and produce less noise. By understanding the pressure and velocity of the air as it flows through an engine, engineers can design engines that are more efficient and produce less noise.\n",
      "In conclusion, Bernoulli's equation is a fundamental principle in fluid mechanics that relates the pressure and velocity of a fluid in motion. It has numerous applications in various fields, including aviation, hydraulics, and engineering. By understanding Bernoulli's equation, engineers and scientists can design more efficient and innovative systems that can improve our daily lives.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=1024)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408ff23-4942-4cba-9d59-6a5ce4078f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
