{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267b2307-b71a-457e-b782-a97a18a36bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f937fe01-4366-4a1a-91fb-fcfcdbef97a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting e2b\n",
      "  Downloading e2b-0.13.15-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aenum>=3.1.11 (from e2b)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: aiohttp>=3.8.4 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (3.9.1)\n",
      "Collecting jsonrpcclient>=4.0.3 (from e2b)\n",
      "  Downloading jsonrpcclient-4.0.3-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: pydantic in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (4.9.0)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (1.26.18)\n",
      "Collecting websockets>=11.0.3 (from e2b)\n",
      "  Downloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from python-dateutil>=2.8.2->e2b) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (2023.11.17)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic->e2b) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic->e2b) (2.14.6)\n",
      "Downloading e2b-0.13.15-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aenum, websockets, jsonrpcclient, e2b\n",
      "Successfully installed aenum-3.1.15 e2b-0.13.15 jsonrpcclient-4.0.3 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install e2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789ea7bb-59ab-4350-bf65-5427ea923821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.336 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (0.0.336)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (0.0.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.336) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.336) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.336) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (2023.11.17)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain==0.0.336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a167dc-3f2f-4658-801a-1ee7232ca8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama_cpp_python\n",
      "Version: 0.2.28\n",
      "Summary: Python bindings for the llama.cpp library\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
      "License: MIT\n",
      "Location: /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages\n",
      "Requires: diskcache, numpy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show llama_cpp_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebefd09c-e51e-41ea-99d7-44deae5130ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.1/426.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b9757e-819c-4688-8a8e-14c859cd2b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1149  100  1149    0     0   2719      0 --:--:-- --:--:-- --:--:--  2722\n",
      "100 3891M  100 3891M    0     0  10.5M      0  0:06:09  0:06:09 --:--:-- 10.8M8k      0  0:06:58  0:00:26  0:06:32 10.3M   0  0:06:51  0:00:29  0:06:22 10.8M    0  9785k      0  0:06:47  0:00:33  0:06:14 10.3M06:25  0:01:02  0:05:23 10.8M6:19  0:01:33  0:04:46 10.8M    0  10.3M      0  0:06:17  0:01:51  0:04:26 10.8M10.3M      0  0:06:16  0:01:53  0:04:23 10.8M0  10.3M      0  0:06:15  0:02:07  0:04:08 10.3M10.4M      0  0:06:13  0:02:35  0:03:38 10.8M  0:06:13  0:02:44  0:03:29 10.8M   0  10.4M      0  0:06:13  0:02:52  0:03:21 10.8M     0  10.4M      0  0:06:12  0:02:56  0:03:16 10.8M    0  10.4M      0  0:06:13  0:02:58  0:03:15 10.3M86M    0     0  10.4M      0  0:06:12  0:03:00  0:03:12 10.3M.4M      0  0:06:12  0:03:14  0:02:58 10.8M 10.4M      0  0:06:12  0:03:21  0:02:51 10.3M    0  10.4M      0  0:06:11  0:03:30  0:02:41 10.3M 0     0  10.4M      0  0:06:11  0:03:40  0:02:31 10.3M     0  10.4M      0  0:06:11  0:03:49  0:02:22 10.3M 0  10.4M      0  0:06:11  0:03:52  0:02:19 10.3M     0  10.5M      0  0:06:10  0:04:22  0:01:48 10.3M    0  10.5M      0  0:06:10  0:04:42  0:01:28 10.3M     0  10.5M      0  0:06:10  0:04:49  0:01:21 10.9M  0     0  10.5M      0  0:06:09  0:05:00  0:01:09 10.8M 0  10.5M      0  0:06:09  0:05:33  0:00:36 10.3M0  10.5M      0  0:06:09  0:05:59  0:00:10 10.8M\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf --output ./models/llama-2-7b-chat.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f83bd6e-6f09-496b-b6f4-8b5ddb305eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d755aa-d5df-48f2-9cc5-18d56f3d26f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5d423-8d11-4dc6-9c64-71fa98eae2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60afdb-38f0-4acd-8664-99d3cb72a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     load_in_4bit=True,\n",
    "#     # config=model_config,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "#     use_auth_token=hf_auth,\n",
    "#     attention_sink_size=4,\n",
    "#     attention_sink_window_size=4092,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca781823-8b32-4626-8841-87509c753060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers\n",
      "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from ctransformers) (0.20.2)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: filelock in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from huggingface-hub->ctransformers) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests->huggingface-hub->ctransformers) (2023.11.17)\n",
      "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-cpuinfo, ctransformers\n",
      "Successfully installed ctransformers-0.2.27 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09957b5b-ce41-4559-8057-de9558f72089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "hf_auth = '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'\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./models/llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=150, context_length=1024, hf=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True, use_auth_token=hf_auth)\n",
    "# tokenizer.save_pretrained(\"./models/tokenizer/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/tokenizer/Llama-2-7b-chat-hf\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89029035-0e0e-4f91-8c72-9c0fc49f67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8307e8-e335-45d6-a9e9-b6477b6c0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a33928b-cafb-4b72-871d-05d4dfa1f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain bernoulli's equation and give examples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb5d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"translate this python code to javascript: print(\"hello world\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e5a106-5f0e-4889-b5bc-23ac46376250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e71b0a-7ef3-46e9-a9c9-e4ec02910c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate this python code to javascript: print(\"hello world\")\n",
      "```\n",
      "You can't directly translate Python code to JavaScript as they are two different programming languages with different syntax and features. However, you can use a Python-to-JavaScript compiler or a JavaScript library that can execute Python code in the browser.\n",
      "Here are a few options:\n",
      "\n",
      "1. `pyjs`: `pyjs` is a Python-to-JavaScript compiler that allows you to run Python code in the browser. You can use it to translate your Python code to JavaScript. Here's an example of how to use `pyjs` to translate the code you provided:\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_length=1024)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147188a7-766b-4c2e-938e-bb21dd937599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "\n",
    "class LlamaLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"llama\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any) -> str:\n",
    "        # if stop is not None:\n",
    "        #     raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        outputs = pipe(prompt, max_new_tokens=512)\n",
    "\n",
    "        return outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2408ff23-4942-4cba-9d59-6a5ce4078f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bot_response\": string  // the human-like response, as a unique string and enclosed with double quotes.\n",
      "\t\"code\": string  // the programming code generated when asked to translate or generate code, as a unique string and enclosed with double quotes. New lines or '\n",
      "' are unescaped.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "bot_response = ResponseSchema(\n",
    "        name=\"bot_response\",\n",
    "        description=\"the human-like response, as a unique string and enclosed with double quotes.\",\n",
    "    )\n",
    "\n",
    "code = ResponseSchema(\n",
    "        name=\"code\",\n",
    "        description=\"the programming code generated when asked to translate or generate code, as a unique string and enclosed with double quotes. New lines or '\\n' are unescaped.\",\n",
    "    )\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [bot_response, code]\n",
    ")\n",
    "\n",
    "response_format = output_parser.get_format_instructions()\n",
    "print(response_format)\n",
    "\n",
    "format_instructions = \"\"\"\n",
    "        The output should be a markdown code snippet strictly formatted in the following schema, including the leading and trailing \"```json\" and \"```\". Keys and values are enclosed with double quotes:\n",
    "\n",
    "        ```json\n",
    "        {\n",
    "        \t\"bot_response\": string  // the human-like response, as a unique string and enclosed with double quotes.\n",
    "        \t\"code\": string  // the programming code generated when asked to translate or generate code, as a unique string and enclosed with double quotes.\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        For python code, enclose code with leading and trailing \"```python\" and \"```\".\n",
    "        For javascript code, enclose code with leading and trailing \"```js\" and \"```\".\n",
    "        For html code, enclose code with leading and trailing \"```html\" and \"```\".\n",
    "        For ruby code, enclose code with leading and trailing \"```ruby\" and \"```\".\n",
    "\n",
    "        Regardless of output, DO NOT escape new lines or \"\\n\".\n",
    "        \"\"\"\n",
    "\n",
    "template_string = \"\"\"You are an expert when it comes to translating programming languages to another. \\\n",
    "        You will be given an instruction and you will start with a human-like greeting or response and ALWAYS SEPARATE THE CODE if there is a code.\n",
    "\n",
    "        Instruction: {query}\n",
    "\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template_string,\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# prompt = PromptTemplate.from_template(template_string)\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\"{query}.\\n Answer as human-like as possible and separate the code if there is a code.\\n {format_instructions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9631f364-00b7-4037-ae4c-c75cb681e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.mrkl.output_parser import MRKLOutputParser\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from typing import Union\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "import signal\n",
    "\n",
    "import e2b\n",
    "from e2b import Sandbox\n",
    "import asyncio\n",
    "\n",
    "\n",
    "llm = LlamaLLM()\n",
    "\n",
    "\n",
    "class InterpreterError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def stdoutIO(stdout=None):\n",
    "    old = sys.stdout\n",
    "    if stdout is None:\n",
    "        stdout = StringIO()\n",
    "    sys.stdout = stdout\n",
    "    yield stdout\n",
    "    sys.stdout = old\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def time_limit(seconds: float):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "\n",
    "    signal.setitimer(signal.ITIMER_REAL, seconds)\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def chdir(root):\n",
    "    if root == \".\":\n",
    "        yield\n",
    "        return\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(root)\n",
    "    try:\n",
    "        yield\n",
    "    except BaseException as exc:\n",
    "        raise exc\n",
    "    finally:\n",
    "        os.chdir(cwd)\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def create_tempdir():\n",
    "    with tempfile.TemporaryDirectory() as dirname:\n",
    "        with chdir(dirname):\n",
    "            yield dirname\n",
    "\n",
    "\n",
    "def my_exec(cmd, globals=None, locals=None, description='source string'):\n",
    "    try:\n",
    "        with stdoutIO() as s:\n",
    "            exec(cmd, globals, locals)\n",
    "\n",
    "        return s.getvalue()\n",
    "\n",
    "    except SyntaxError as err:\n",
    "        error_class = err.__class__.__name__\n",
    "        detail = err.args[0]\n",
    "        line_number = err.lineno\n",
    "        raise Exception(\"%s at line %d of %s: %s\" % (error_class, line_number, description, detail))\n",
    "\n",
    "    except Exception as err:\n",
    "        error_class = err.__class__.__name__\n",
    "        detail = err.args[0]\n",
    "        cl, exc, tb = sys.exc_info()\n",
    "        line_number = traceback.extract_tb(tb)[-1][1]\n",
    "        raise Exception(\"%s at line %d of %s: %s\" % (error_class, line_number, description, detail))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_query(query):\n",
    "    \"\"\"\n",
    "    Formats query with a template instruction.\n",
    "    \"\"\"\n",
    "    # formatted_output = prompt.format(**{\"query\":query, \"format_instructions\":output_parser.get_format_instructions()})\n",
    "    formatted_output = prompt.format(query=query)\n",
    "    # formatted_output = prompt.format_prompt(query=query)\n",
    "    return formatted_output\n",
    "\n",
    "# async def run_js(code: str):\n",
    "#     #\n",
    "#     session = await e2b.Session.create(id=\"Nodejs\")\n",
    "#     await session.filesystem.write(\"/home/user/index.js\", code)\n",
    "#     # 2. Then execute the file with Node.\n",
    "#     proc = await session.process.start(\"node /home/user/index.js\")\n",
    "#     # 3. Wait for the process to finish.\n",
    "#     out = await proc\n",
    "#     # 4. Return the stdout and stderr.\n",
    "#     return out.stdout, out.stderr\n",
    "\n",
    "def run_code(response_palm, timeout: float = 3.0, tmp_dir: str = None,):\n",
    "    \"\"\"\n",
    "    Evaluates the functional correctness of a completion by running the test\n",
    "    suite provided in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    random_id = random.uniform(1, 1000)\n",
    "\n",
    "\n",
    "    try:\n",
    "      parsed_output = output_parser.parse(response_palm)\n",
    "    except:\n",
    "      parsed_response_palm = re.sub(r\"(?<!\\\\)\\\\'\", \"'\", response_palm)\n",
    "      parsed_output = output_parser.parse(parsed_response_palm)\n",
    "\n",
    "\n",
    "    if 'code' in parsed_output and parsed_output['code'] and parsed_output['code'] != \"\" and \"python\" in parsed_output['code']:\n",
    "\n",
    "      code = parsed_output['code']\n",
    "\n",
    "      if \"```python\" in code:\n",
    "          code = code.replace('```python', '')\n",
    "\n",
    "      if \"```\" in code[len(code)-5:]:\n",
    "          code = code.replace('```', '')\n",
    "\n",
    "      python_exec = None\n",
    "      try:\n",
    "        python_exec = my_exec(code)\n",
    "\n",
    "      except Exception as e:\n",
    "        python_exec = str(e)\n",
    "\n",
    "\n",
    "\n",
    "      out = python_exec\n",
    "      in_append = str(parsed_output['bot_response']) if parsed_output['bot_response'] else \"\"\n",
    "      out_append = \"\\n\\n\\n Output from sandbox runtime: \" + str(out) if out else \"\\n\\n\\n No output expected.\"\n",
    "      code = code if code not in parsed_output['bot_response'] else \"\"\n",
    "      code_start = \"\\n```python\\n\" if code and \"```python\" not in code else \"\"\n",
    "      code_end = \"\\n```\\n\" if code and \"```\" not in code[len(code)-5:] else \"\"\n",
    "      print(\"Code end:\", code_end)\n",
    "      res = in_append + code_start + code + code_end + out_append\n",
    "\n",
    "    elif 'code' in parsed_output and parsed_output['code'] and parsed_output['code'] != \"\" and \"js\" in parsed_output['code']:\n",
    "        tmp_dir = \"/\"\n",
    "        code = parsed_output['code']\n",
    "\n",
    "        if \"```js\" in code:\n",
    "          code = code.replace('```js', '')\n",
    "\n",
    "        if \"```\" in code[len(code)-5:]:\n",
    "          code = code.replace('```', '')\n",
    "\n",
    "\n",
    "        sandbox = Sandbox(api_key=\"e2b_29a1369676909e50be7dcfede4120ec8d4a72de7\")\n",
    "        sandbox.filesystem.write(\"/home/user/test.js\", code)\n",
    "        # if \"tmp\" not in tmp_dir:\n",
    "        #     tmp_dir = os.path.join(tmp_dir, \"tmp\")\n",
    "        # tmp_dir = os.path.join(tmp_dir, f\"{random_id}\")\n",
    "        # if not os.path.exists(tmp_dir):\n",
    "        #     os.makedirs(tmp_dir)\n",
    "\n",
    "        # os.chdir(tmp_dir)\n",
    "        # open(f\"test.js\", 'w').write(code)\n",
    "        try:\n",
    "            # exec_result = None\n",
    "            with time_limit(timeout):\n",
    "              # WARNING\n",
    "              # This program exists to execute untrusted model-generated code. Although\n",
    "              # it is highly unlikely that model-generated code will do something overtly\n",
    "              # malicious in response to this test suite, model-generated code may act\n",
    "              # destructively due to a lack of model capability or alignment.\n",
    "              # Users are strongly encouraged to sandbox this evaluation suite so that it\n",
    "              # does not perform destructive actions on their host or network.\n",
    "              # Once you have read this disclaimer and taken appropriate precautions,\n",
    "              # uncomment the following line and proceed at your own risk:\n",
    "              exec_result = sandbox.process.start(\"node test.js\")\n",
    "              exec_result.wait()\n",
    "              # exec_result = subprocess.run([\"node\", \"test.js\"], timeout=timeout, capture_output=True)\n",
    "\n",
    "            # if exec_result.stderr.decode():\n",
    "            #     err = exec_result.stderr.decode()\n",
    "            #     raise Exception(f\"failed: {err}\")\n",
    "            # elif exec_result.stdout.decode():\n",
    "            #     out = exec_result.stdout.decode()\n",
    "            # else:\n",
    "            #     out = \"passed\"\n",
    "            if exec_result.stderr:\n",
    "                err = exec_result.stderr\n",
    "                raise Exception(f\"failed: {err}\")\n",
    "            elif exec_result.stdout:\n",
    "                out = exec_result.stdout\n",
    "            else:\n",
    "                out = \"passed\"\n",
    "\n",
    "\n",
    "        except TimeoutException:\n",
    "            out = \"timed out\"\n",
    "\n",
    "        except Exception as e:\n",
    "            out = str(e)\n",
    "\n",
    "        in_append = str(parsed_output['bot_response']) if parsed_output['bot_response'] else \"\"\n",
    "        out_append = \"\\n\\n\\n Output from sandbox runtime: \" + str(out) if out else \"\\n\\n\\n No output expected.\"\n",
    "        code = code if code not in parsed_output['bot_response'] else \"\"\n",
    "        code_start = \"\\n```js\\n\" if code and \"```js\" not in code else \"\"\n",
    "        code_end = \"\\n```\\n\" if code and \"```\" not in code[len(code)-5:] else \"\"\n",
    "        print(\"Code end:\", code_end)\n",
    "        res = in_append + code_start + code + code_end + out_append\n",
    "\n",
    "        # shutil.rmtree(tmp_dir)\n",
    "        sandbox.close()\n",
    "    else:\n",
    "        res = parsed_output['bot_response']\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d882d9-76be-4d1a-bf59-58c5d2886872",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = format_query(\"\"\"translate this python code to javascript: print(\"hello world\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a7d916-fafa-44dc-96c5-8f95b58b029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = format_query(\"\"\"hey\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2e6520-8a10-4c2e-bfec-d8f67be09f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert when it comes to translating programming languages to another.         You will be given an instruction and you will start with a human-like greeting or response and ALWAYS SEPARATE THE CODE if there is a code.\n",
      "\n",
      "        Instruction: translate this python code to javascript: print(\"hello world\")\n",
      "\n",
      "        \n",
      "        The output should be a markdown code snippet strictly formatted in the following schema, including the leading and trailing \"```json\" and \"```\". Keys and values are enclosed with double quotes:\n",
      "\n",
      "        ```json\n",
      "        {\n",
      "        \t\"bot_response\": string  // the human-like response, as a unique string and enclosed with double quotes.\n",
      "        \t\"code\": string  // the programming code generated when asked to translate or generate code, as a unique string and enclosed with double quotes.\n",
      "        }\n",
      "        ```\n",
      "\n",
      "        For python code, enclose code with leading and trailing \"```python\" and \"```\".\n",
      "        For javascript code, enclose code with leading and trailing \"```js\" and \"```\".\n",
      "        For html code, enclose code with leading and trailing \"```html\" and \"```\".\n",
      "        For ruby code, enclose code with leading and trailing \"```ruby\" and \"```\".\n",
      "\n",
      "        Regardless of output, DO NOT escape new lines or \"\n",
      "\".\n",
      "        \n",
      "        \n",
      "        Please provide your answer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_out = llm(_input)\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8b895a-36c6-4605-a74c-7f5f54dde9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert when it comes to translating programming languages to another.         You will be given an instruction and you will start with a human-like greeting or response and ALWAYS SEPARATE THE CODE if there is a code.\\n\\n        Instruction: translate this python code to javascript: print(\"hello world\")\\n\\n        \\n        The output should be a markdown code snippet strictly formatted in the following schema, including the leading and trailing \"```json\" and \"```\". Keys and values are enclosed with double quotes:\\n\\n        ```json\\n        {\\n        \\t\"bot_response\": string  // the human-like response, as a unique string and enclosed with double quotes.\\n        \\t\"code\": string  // the programming code generated when asked to translate or generate code, as a unique string and enclosed with double quotes.\\n        }\\n        ```\\n\\n        For python code, enclose code with leading and trailing \"```python\" and \"```\".\\n        For javascript code, enclose code with leading and trailing \"```js\" and \"```\".\\n        For html code, enclose code with leading and trailing \"```html\" and \"```\".\\n        For ruby code, enclose code with leading and trailing \"```ruby\" and \"```\".\\n\\n        Regardless of output, DO NOT escape new lines or \"\\n\".\\n        \\n        \\n        Please provide your answer.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dead00f-671e-4b9b-9469-677f6b768339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
