{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267b2307-b71a-457e-b782-a97a18a36bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab2880a-ff55-4c8e-8103-af34ab2075be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.336\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages\n",
      "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f937fe01-4366-4a1a-91fb-fcfcdbef97a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting e2b\n",
      "  Downloading e2b-0.13.15-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aenum>=3.1.11 (from e2b)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: aiohttp>=3.8.4 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (3.9.1)\n",
      "Collecting jsonrpcclient>=4.0.3 (from e2b)\n",
      "  Downloading jsonrpcclient-4.0.3-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: pydantic in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (4.9.0)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from e2b) (1.26.18)\n",
      "Collecting websockets>=11.0.3 (from e2b)\n",
      "  Downloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp>=3.8.4->e2b) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from python-dateutil>=2.8.2->e2b) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests>=2.31.0->e2b) (2023.11.17)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic->e2b) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic->e2b) (2.14.6)\n",
      "Downloading e2b-0.13.15-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: aenum, websockets, jsonrpcclient, e2b\n",
      "Successfully installed aenum-3.1.15 e2b-0.13.15 jsonrpcclient-4.0.3 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install e2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789ea7bb-59ab-4350-bf65-5427ea923821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.336 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (0.0.336)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (3.9.1)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (0.0.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.0.336) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.336) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.336) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.336) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.336) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.336) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.336) (2023.11.17)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.336) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain==0.0.336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a167dc-3f2f-4658-801a-1ee7232ca8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama_cpp_python\n",
      "Version: 0.2.28\n",
      "Summary: Python bindings for the llama.cpp library\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
      "License: MIT\n",
      "Location: /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages\n",
      "Requires: diskcache, numpy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show llama_cpp_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebefd09c-e51e-41ea-99d7-44deae5130ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.36.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b9757e-819c-4688-8a8e-14c859cd2b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1152  100  1152    0     0   1136      0  0:00:01  0:00:01 --:--:--  1137\n",
      "  4 3263M    4  135M    0     0   276k      0  3:21:05  0:08:22  3:12:43     00  2:55:01  0:00:14  2:54:47  308k   0   403k      0  2:18:10  0:00:54  2:17:16 78945 0  2:22:09  0:01:24  2:20:45  498k     0   415k      0  2:14:03  0:01:37  2:12:26  584kk      0  2:10:46  0:01:47  2:08:59  422k20:20  0:01:55  2:18:25     0 0   497k      0  1:51:58  0:02:38  1:49:20  650k:31  1:27:42  614k2  0:03:32  1:27:40  624k    0   607k      0  1:31:42  0:03:39  1:28:03  503k0   530k      0  1:45:04  0:04:22  1:40:42     0  0     0   512k      0  1:48:42  0:04:31  1:44:11     00     0   487k      0  1:54:19  0:04:45  1:49:34     069k      0  1:58:44  0:04:56  1:53:48     0k      0  1:59:56  0:04:59  1:54:57     0      0  2:01:33  0:05:03  1:56:30     0     0  2:05:34  0:05:13  2:00:21     05M    0     0   425k      0  2:10:47  0:05:26  2:05:21     0     0   419k      0  2:12:48  0:05:31  2:07:17     0      0  2:13:12  0:05:32  2:07:40     0 0     0   411k      0  2:15:12  0:05:37  2:09:35     0  0     0   407k      0  2:16:49  0:05:41  2:11:08     0k      0  2:24:27  0:06:00  2:18:27     00   358k      0  2:35:18  0:06:28  2:28:50     048k      0  2:39:43  0:06:39  2:33:04     0     0  2:42:07  0:06:45  2:35:22     006:57  2:39:59     023k      0  2:52:10  0:07:10  2:45:00     05k      0  3:02:12  0:07:35  2:54:37     0  3:03:24  0:07:38  2:55:46     0      0  3:12:14  0:08:00  3:04:14     081k      0  3:17:52  0:08:14  3:09:38     0^C\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf --output ./models/llama-2-7b-chat.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f83bd6e-6f09-496b-b6f4-8b5ddb305eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./models/llama-2-7b-chat.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d755aa-d5df-48f2-9cc5-18d56f3d26f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7799.70 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    40 runs   (    0.12 ms per token,  8629.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7799.55 ms /    15 tokens (  519.97 ms per token,     1.92 tokens per second)\n",
      "llama_print_timings:        eval time =  314226.40 ms /    39 runs   ( 8057.09 ms per token,     0.12 tokens per second)\n",
      "llama_print_timings:       total time =  322142.09 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a5d423-8d11-4dc6-9c64-71fa98eae2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-a8e312ce-4ba0-4ef4-8c90-33aa81c39fc1', 'object': 'text_completion', 'created': 1734163267, 'model': './models/llama-2-7b-chat.Q4_K_M.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1. Mercury 2. Venus 3. Earth 4. Mars 5. Jupiter 6. Saturn 7. Uranus 8. Neptune', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 40, 'total_tokens': 55}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60afdb-38f0-4acd-8664-99d3cb72a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     trust_remote_code=True,\n",
    "#     load_in_4bit=True,\n",
    "#     # config=model_config,\n",
    "#     # quantization_config=bnb_config,\n",
    "#     device_map='auto',\n",
    "#     use_auth_token=hf_auth,\n",
    "#     attention_sink_size=4,\n",
    "#     attention_sink_window_size=4092,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca781823-8b32-4626-8841-87509c753060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx\n",
      "  Downloading mlx-0.21.1-cp39-cp39-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Downloading mlx-0.21.1-cp39-cp39-macosx_14_0_arm64.whl (27.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: mlx\n",
      "Successfully installed mlx-0.21.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e3e4d31-953d-408d-85b6-441078f96cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.2\n",
      "    Uninstalling protobuf-4.25.2:\n",
      "      Successfully uninstalled protobuf-4.25.2\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c660d6-45eb-46d9-bf04-afd965f025e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.2.5\n",
      "  Using cached langchain-0.2.5-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.7 (from langchain==0.2.5)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.5)\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.5)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain==0.2.5) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (1.33)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (4.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5)\n",
      "  Downloading orjson-3.10.12-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.2.5) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.2.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.2.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.2.5) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.2.5) (2023.11.17)\n",
      "Requirement already satisfied: anyio in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (3.5.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (2.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.2.0)\n",
      "Using cached langchain-0.2.5-py3-none-any.whl (974 kB)\n",
      "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.12-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: packaging, orjson, h11, requests-toolbelt, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.81\n",
      "    Uninstalling langsmith-0.0.81:\n",
      "      Successfully uninstalled langsmith-0.0.81\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.336\n",
      "    Uninstalling langchain-0.0.336:\n",
      "      Successfully uninstalled langchain-0.0.336\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 langchain-0.2.5 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 orjson-3.10.12 packaging-24.2 requests-toolbelt-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb6c5612-c7dd-4661-98f0-022023bcf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "as a physics expert, explain fourier analysis to a five yr old child. you will be provided some context to help you answer. \\n context: \n",
    "\n",
    "Bass guitar time signal of open string A note (55 Hz).\n",
    "\n",
    "Fourier transform of bass guitar time signal of open string A note (55 Hz). Fourier analysis reveals the oscillatory components of signals and functions.\n",
    "Fourier transforms\n",
    "Fourier transform\n",
    "Fourier series\n",
    "Discrete-time Fourier transform\n",
    "Discrete Fourier transform\n",
    "Discrete Fourier transform over a ring\n",
    "Fourier transform on finite groups\n",
    "Fourier analysis\n",
    "Related transforms\n",
    "In mathematics, Fourier analysis (/ˈfʊrieɪ, -iər/)[1] is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer.\n",
    "\n",
    "The subject of Fourier analysis encompasses a vast spectrum of mathematics. In the sciences and engineering, the process of decomposing a function into oscillatory components is often called Fourier analysis, while the operation of rebuilding the function from these pieces is known as Fourier synthesis. For example, determining what component frequencies are present in a musical note would involve computing the Fourier transform of a sampled musical note. One could then re-synthesize the same sound by including the frequency components as revealed in the Fourier analysis. In mathematics, the term Fourier analysis often refers to the study of both operations.\n",
    "\n",
    "The decomposition process itself is called a Fourier transformation. Its output, the Fourier transform, is often given a more specific name, which depends on the domain and other properties of the function being transformed. Moreover, the original concept of Fourier analysis has been extended over time to apply to more and more abstract and general situations, and the general field is often known as harmonic analysis. Each transform used for analysis (see list of Fourier-related transforms) has a corresponding inverse transform that can be used for synthesis.\n",
    "\n",
    "To use Fourier analysis, data must be equally spaced. Different approaches have been developed for analyzing unequally spaced data, notably the least-squares spectral analysis (LSSA) methods that use a least squares fit of sinusoids to data samples, similar to Fourier analysis.[2][3] Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long gapped records; LSSA mitigates such problems.[4]\n",
    "\n",
    "Applications\n",
    "Fourier analysis has many scientific applications – in physics, partial differential equations, number theory, combinatorics, signal processing, digital image processing, probability theory, statistics, forensics, option pricing, cryptography, numerical analysis, acoustics, oceanography, sonar, optics, diffraction, geometry, protein structure analysis, and other areas.\n",
    "\n",
    "This wide applicability stems from many useful properties of the transforms:\n",
    "\n",
    "The transforms are linear operators and, with proper normalization, are unitary as well (a property known as Parseval's theorem or, more generally, as the Plancherel theorem, and most generally via Pontryagin duality).[5]\n",
    "The transforms are usually invertible.\n",
    "The exponential functions are eigenfunctions of differentiation, which means that this representation transforms linear differential equations with constant coefficients into ordinary algebraic ones.[6] Therefore, the behavior of a linear time-invariant system can be analyzed at each frequency independently.\n",
    "By the convolution theorem, Fourier transforms turn the complicated convolution operation into simple multiplication, which means that they provide an efficient way to compute convolution-based operations such as signal filtering, polynomial multiplication, and multiplying large numbers.[7]\n",
    "The discrete version of the Fourier transform (see below) can be evaluated quickly on computers using fast Fourier transform (FFT) algorithms.[8]\n",
    "In forensics, laboratory infrared spectrophotometers use Fourier transform analysis for measuring the wavelengths of light at which a material will absorb in the infrared spectrum. The FT method is used to decode the measured signals and record the wavelength data. And by using a computer, these Fourier calculations are rapidly carried out, so that in a matter of seconds, a computer-operated FT-IR instrument can produce an infrared absorption pattern comparable to that of a prism instrument.[9]\n",
    "\n",
    "Fourier transformation is also useful as a compact representation of a signal. For example, JPEG compression uses a variant of the Fourier transformation (discrete cosine transform) of small square pieces of a digital image. The Fourier components of each square are rounded to lower arithmetic precision, and weak components are eliminated, so that the remaining components can be stored very compactly. In image reconstruction, each image square is reassembled from the preserved approximate Fourier-transformed components, which are then inverse-transformed to produce an approximation of the original image.\n",
    "\n",
    "In signal processing, the Fourier transform often takes a time series or a function of continuous time, and maps it into a frequency spectrum. That is, it takes a function from the time domain into the frequency domain; it is a decomposition of a function into sinusoids of different frequencies; in the case of a Fourier series or discrete Fourier transform, the sinusoids are harmonics of the fundamental frequency of the function being analyzed.\n",
    "\n",
    "When a function \n",
    "s\n",
    "(\n",
    "t\n",
    ")\n",
    "{\\displaystyle s(t)} is a function of time and represents a physical signal, the transform has a standard interpretation as the frequency spectrum of the signal. The magnitude of the resulting complex-valued function \n",
    "S\n",
    "(\n",
    "f\n",
    ")\n",
    "{\\displaystyle S(f)} at frequency \n",
    "f\n",
    "{\\displaystyle f} represents the amplitude of a frequency component whose initial phase is given by the angle of \n",
    "S\n",
    "(\n",
    "f\n",
    ")\n",
    "{\\displaystyle S(f)} (polar coordinates).\n",
    "\n",
    "Fourier transforms are not limited to functions of time, and temporal frequencies. They can equally be applied to analyze spatial frequencies, and indeed for nearly any function domain. This justifies their use in such diverse branches as image processing, heat conduction, and automatic control.\n",
    "\n",
    "When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate narrowband components of a compound waveform, concentrating them for easier detection or removal. A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[10]\n",
    "\n",
    "Some examples include:\n",
    "\n",
    "Equalization of audio recordings with a series of bandpass filters;\n",
    "Digital radio reception without a superheterodyne circuit, as in a modern cell phone or radio scanner;\n",
    "Image processing to remove periodic or anisotropic artifacts such as jaggies from interlaced video, strip artifacts from strip aerial photography, or wave patterns from radio frequency interference in a digital camera;\n",
    "Cross correlation of similar images for co-alignment;\n",
    "X-ray crystallography to reconstruct a crystal structure from its diffraction pattern;\n",
    "Fourier-transform ion cyclotron resonance mass spectrometry to determine the mass of ions from the frequency of cyclotron motion in a magnetic field;\n",
    "Many other forms of spectroscopy, including infrared and nuclear magnetic resonance spectroscopies;\n",
    "Generation of sound spectrograms used to analyze sounds;\n",
    "Passive sonar used to classify targets based on machinery noise.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "147188a7-766b-4c2e-938e-bb21dd937599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 4096.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from /Users/cuburtbalanon/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf\n",
      "8 bits quantized model\n",
      "Model info\n",
      "==========\n",
      "Context length: 2048\n",
      "Vocab size: 32000\n",
      "Hidden size: 2048\n",
      "Num layers: 22\n",
      "Num attention heads: 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from gguf_llm.generate import generate\n",
    "from gguf_llm import models\n",
    "import mlx.core as mx\n",
    "\n",
    "class TinyLlama:\n",
    "    def __init__(self):\n",
    "        mx.random.seed(1)\n",
    "        self.model, self.tokenizer = models.load(\"tinyllama-1.1b-chat-v1.0.Q8_0.gguf\", \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\")\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int, temp: float = 0.0):\n",
    "        # Encode the prompt\n",
    "        prompt_encoded = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        # Initialization\n",
    "        tic = time.time()\n",
    "        tokens = []\n",
    "        skip = 0\n",
    "        for token, n in zip(\n",
    "            models.generate(prompt_encoded, self.model, temp),\n",
    "            range(max_tokens),\n",
    "        ):\n",
    "            if token == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "            if n == 0:\n",
    "                prompt_time = time.time() - tic\n",
    "                tic = time.time()\n",
    "        \n",
    "            tokens.append(token.item())\n",
    "            # s = tokenizer.decode(tokens)\n",
    "            # print(s[skip:], end=\"\", flush=True)\n",
    "            # skip = len(s)\n",
    "                \n",
    "        print(self.tokenizer.decode(tokens)[skip:], flush=True)\n",
    "        # gen_time = time.time() - tic\n",
    "        # print(\"=\" * 10)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            print(\"No tokens generated for this prompt\")\n",
    "            \n",
    "        # prompt_tps = prompt_encoded.size / prompt_time\n",
    "        # gen_tps = (len(tokens) - 1) / gen_time\n",
    "        # print(f\"Prompt: {prompt_tps:.3f} tokens-per-sec\")\n",
    "        # print(f\"Generation: {gen_tps:.3f} tokens-per-sec\")\n",
    "        return self.tokenizer.decode(tokens)[skip:]\n",
    "\n",
    "model = TinyLlama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0676ba9-5e85-4710-ae0d-5b2d5c278bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLlamaLLM(LLM):\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"tinyllama\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any) -> str:\n",
    "        # if stop is not None:\n",
    "        #     raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        outputs = model.generate(prompt=str(prompt), max_tokens=256, temp=0.3)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1c2e6520-8a10-4c2e-bfec-d8f67be09f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier transforms are used in many fields, including:\n",
      "\n",
      "Computer science: Fourier transforms are used in signal processing, image processing, and digital signal processing.\n",
      "Statistics: Fourier transforms are used to analyze time series and to generate spectral statistics.\n",
      "Mathematics: Fourier transforms are used in mathematical analysis, such as Fourier series, Fourier analysis, and Fourier transforms over finite groups.\n",
      "Physics: Fourier transforms are used in physics, such as Fourier\n"
     ]
    }
   ],
   "source": [
    "llm_out = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dead00f-671e-4b9b-9469-677f6b768339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
