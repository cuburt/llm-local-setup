{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8503bcb7-1813-4ace-8fa9-b446c0868ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f730300-a18c-4d31-af7e-11e514bfa983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade-strategy eager optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55600bc6-3efd-4214-ae70-31026195653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d815e0-391b-4d1c-841c-d9066e239452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec3b876-ed47-4975-9a17-c69674d080de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yahoo_answers_topics\", split=\"test\").shuffle().select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff451717-0cb7-4d35-b0d2-588586537899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.39k/1.39k [00:00<?, ?B/s]\n",
      "Framework not specified. Using pt to export to ONNX.\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.43G/1.43G [01:10<00:00, 20.3MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 968kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 672kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 772/772 [00:00<?, ?B/s]\n",
      "Using framework PyTorch: 2.0.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:239: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:246: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:278: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:936: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1558: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/distilled/onnx\\\\tokenizer_config.json',\n",
       " 'tmp/distilled/onnx\\\\special_tokens_map.json',\n",
       " 'tmp/distilled/onnx\\\\vocab.json',\n",
       " 'tmp/distilled/onnx\\\\merges.txt',\n",
       " 'tmp/distilled/onnx\\\\added_tokens.json',\n",
       " 'tmp/distilled/onnx\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_type = \"zero-shot-classification\"\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "# evaluator = evaluate.evaluator(task_type)\n",
    "save_directory = \"tmp/distilled/onnx\"\n",
    "model_id = \"valhalla/distilbart-mnli-12-9\"\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf0f670-ee0d-4bff-a9ac-5f07c93e48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task_type, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0ca7b4-f6f9-4f06-852a-d69dc2891f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuburt.balanon\\AppData\\Roaming\\Python\\Python311\\site-packages\\optimum\\onnxruntime\\configuration.py:765: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "Optimizing model...\n",
      "Configuration saved in tmp\\distilled\\onnx\\ort_config.json\n",
      "Optimized model saved at: tmp\\distilled\\onnx (external data format: False; saved all tensor to one file: True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('tmp/distilled/onnx')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "optimizer.optimize(OptimizationConfig(optimization_level=99), save_dir=save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30bf065-f674-4402-b9ba-d93b3de1a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimized = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"model_optimized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b729c57b-1950-48b9-b456-9b4488bd034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_optimized = pipeline(task_type, model=model_optimized, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8774829b-809a-4dab-8674-de7ae7847925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec03ba1-146d-4a89-8f29-9365ba485ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: True)\n",
      "Quantizing model...\n",
      "Saving quantized model at: tmp\\distilled\\onnx (external data format: False)\n",
      "Configuration saved in tmp\\distilled\\onnx\\ort_config.json\n"
     ]
    }
   ],
   "source": [
    "quantizer = ORTQuantizer.from_pretrained(model_optimized)\n",
    "qconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=True, reduce_range=True)\n",
    "quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)\n",
    "model_quantized = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"model_quantized.onnx\")\n",
    "classifier_quantized = pipeline(task_type, model=model_quantized, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c5901d-5185-4f22-a0d7-3ee0d72d35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_pipeline(pipeline):\n",
    "    y_pred = []\n",
    "    speed = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        start_time = time.time()*1000\n",
    "        res = pipeline(row[\"question_title\"], candidate_labels)\n",
    "        end_time = time.time()*1000\n",
    "        pred = res['labels'][res['scores'].index(max(res['scores']))]\n",
    "        latency = end_time - start_time\n",
    "        # print('Inference ', i, '\\n', row[\"question_title\"], '\\nPrediction: ', pred, '\\nLabel: ', dataset.features[\"topic\"].int2str(i), '\\nLatency: ', latency)\n",
    "        y_pred.append(pred)\n",
    "        speed.append(latency)\n",
    "    print(np.mean(speed))\n",
    "    print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91337d2c-81f6-4ecf-aa56-59b0a65d4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = dataset.features[\"topic\"].int2str(set(dataset[\"topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413ffcfb-30f8-4969-8ac2-ecc9d78e08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dataset.features[\"topic\"].int2str(dataset[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93910851-c07d-40c6-a2d0-480aab34ad76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081.77134375\n",
      "0.472\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4df9eb9-48fc-4e97-b88c-7171eef995cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.9123334960938\n",
      "0.472\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(classifier_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdcb05e7-73c3-43d9-9a1c-8fc46af853fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527.645947265625\n",
      "0.274\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(classifier_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514cfd3-28c0-49ee-ab2d-05e6dea36c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu-inference]",
   "language": "python",
   "name": "conda-env-cpu-inference-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
