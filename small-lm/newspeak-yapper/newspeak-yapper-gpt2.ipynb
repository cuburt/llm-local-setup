{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23338b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains the base Tokenizer class and a few common helper functions.\n",
    "The base class also contains the (common) save/load functionality.\n",
    "It would be possible to be a lot more strict about the interface and\n",
    "e.g. isolating all regex/pattern parts to the RegexTokenizer, but\n",
    "some concessions are made for simplicity.\n",
    "\"\"\"\n",
    "import unicodedata\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# a few helper functions useful for both BasicTokenizer and RegexTokenizer\n",
    "\n",
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# first two helper functions...\n",
    "def replace_control_characters(s: str) -> str:\n",
    "    # we don't want to print control characters\n",
    "    # which distort the output (e.g. \\n or much worse)\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
    "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# the base Tokenizer class\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char ï¿½.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0d58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal (byte-level) Byte Pair Encoding tokenizer.\n",
    "\n",
    "Algorithmically follows along the GPT tokenizer:\n",
    "https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "\n",
    "Unlike BasicTokenizer:\n",
    "- RegexTokenizer handles an optional regex splitting pattern.\n",
    "- RegexTokenizer handles optional special tokens.\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # split the text up into text chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunk_ids, stats)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "        if none_raise, then an error is raised if any special token is encountered in text\n",
    "        this is the default tiktoken behavior right now as well\n",
    "        any other behavior is either annoying, or a major footgun\n",
    "        \"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        # otherwise, we have to be careful with potential special tokens in text\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d214616e-9ad9-447b-8ec8-42f34706e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "from datasets import load_dataset\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, dataset=None):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.tokenizer = RegexTokenizer()\n",
    "        self.data_dir = os.path.join('data', dataset)\n",
    "        self.config = Config()\n",
    "\n",
    "        # if dataset == 'newspeak400':\n",
    "        #     vocab_size = 488\n",
    "        #     df = pd.read_csv(\"newspeak.csv\")\n",
    "        #     self.text = \"\".join([v for v in df[\"Newspeak Translation\"]])\n",
    "            \n",
    "        #     if not os.path.exists(os.path.join(data_dir, f\"{dataset}.model\")):\n",
    "        #         DATA_CACHE_DIR = os.path.join(os.path.abspath(__file__), data_dir)\n",
    "        #         os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "        #         self.tokenizer.train(self.text, vocab_size=vocab_size, verbose=True) # 9746 is the size of the vocabulary\n",
    "        #         self.tokenizer.save(os.path.join(data_dir, dataset))\n",
    "            \n",
    "        # elif dataset == 'fineweb_edu10b':\n",
    "        #     vocab_size = 488\n",
    "        #     if not os.path.exists(os.path.join(data_dir, f\"{dataset}.model\")):\n",
    "        #         DATA_CACHE_DIR = os.path.join(os.path.abspath(__file__), data_dir)\n",
    "        #         os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "        #         self.tokenizer.register_special_tokens({\"<|endoftext|>\": vocab_size+1})\n",
    "        #         self.tokenizer.train(self.text, vocab_size=vocab_size, verbose=True)\n",
    "        #         self.tokenizer.save(os.path.join(data_dir, dataset))\n",
    "\n",
    "        # self.tokenizer.load(f\"{dataset}.model\") # loads the model back from disk\n",
    "        \n",
    "        # self.tokens = torch.tensor(self.tokenizer.encode(self.text), dtype=torch)\n",
    "        # print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        # print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        self.curent_idx = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        # example batch of 4 sequences of length 6. we'll use the first 24 tokens as input, \n",
    "        # and the last 24 tokens as labels. this is a common trick in language modeling, \n",
    "        # where we want to predict the next token in the sequence.\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.curent_idx:self.curent_idx + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.curent_idx += B * T\n",
    "        if self.curent_idx + B * T > len(self.tokens):\n",
    "            self.curent_idx = 0\n",
    "        return x, y\n",
    "    \n",
    "    def get_batch(self, split):\n",
    "        if split == 'train':\n",
    "            data = np.memmap(os.path.join(self.data_dir, \"train.bin\"), dtype=np.uint16, mode='r')\n",
    "        else:\n",
    "            data = np.memmap(os.path.join(self.data_dir, \"val.bin\"), dtype=np.uint16, mode='r')\n",
    "        ix = torch.randint(len(data) - self.config.context_length, (self.B,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i + self.config.context_length]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i + 1:i + self.config.context_length + 1]).astype(np.int64)) for i in ix])\n",
    "        if self.config.device == 'cuda':\n",
    "            x, y = x.pin_memory().to(self.config.device, non_blocking=True), y.pin_memory().to(self.config.device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(self.config.device), y.to(self.config.device)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class Config:\n",
    "    device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    vocab_size: int = 338025\n",
    "    n_embed: int = 256\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 6\n",
    "    context_length: int = 128\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True \n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "            wpe = nn.Embedding(config.context_length, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embed, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight # tie weights | weight sharing scheme\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                # this is the projection layer of the attention head\n",
    "                # we want to initialize it with the same weights as the embedding layer\n",
    "                # multiply n_layers by 2, because we have 2 additive connections in the block\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layers))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        If non_embedding is True, it excludes the embedding layer.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name):\n",
    "        # load pretrained model from huggingface\n",
    "        assert model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        config_args = {\n",
    "            \"gpt2\": {\"n_embed\": 768, \"n_heads\": 12, \"n_layers\": 12},\n",
    "            \"gpt2-medium\": {\"n_embed\": 1024, \"n_heads\": 16, \"n_layers\": 24},\n",
    "            \"gpt2-large\": {\"n_embed\": 1280, \"n_heads\": 20, \"n_layers\": 36},\n",
    "            \"gpt2-xl\": {\"n_embed\": 1600, \"n_heads\": 25, \"n_layers\": 48},\n",
    "        }[model_name]\n",
    "        config_args[\"context_length\"] = 1024\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config = Config(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = [k for k in sd.keys() if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        # remove the extra keys from the huggingface model\n",
    "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith(\".attn.bias\") or not k.endswith(\".attn.masked_bias\")]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        for k in sd_keys_hf:\n",
    "            if k not in sd_keys:\n",
    "                print(f\"Warning: {k} not in model state dict\")\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"Mismatch in number of keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch for {k}: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "        \n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch for {k}: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # B, T\n",
    "        # make sure the input is within the context length\n",
    "        assert T <= self.config.context_length, f\"Cannot forward, model block size is {self.config.context_length}, but input is {T}\"\n",
    "        # forward the token embeddings and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # positional embedding of shape (T, n_embed)\n",
    "        tok_emb = self.transformer.wte(idx) # token embedding of shape (B, T, n_embed)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layer norm and the lm head\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :]) # shape (B, 1, vocab_size)\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # create optim groups. any parameters that is 2D will be weight delayed, otherwise not\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay , all biases and layernorms dont\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"number of decay params: {num_decay_params / 1e6:.2f}M\")\n",
    "        print(f\"number of nodecay params: {num_nodecay_params / 1e6:.2f}M\")\n",
    "        # create the optimizer and use fused version if available \n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        print(\"using fused adamw\" if use_fused else \"using adamw\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.context_length else idx[:, -self.config.context_length:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm with optioanl bias\"\"\"\n",
    "\n",
    "    def __init__(self, n_dim, bias):\n",
    "        super().__init__()\n",
    "        self.weigth = nn.Parameter(torch.ones(n_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weigth.shape, self.weigth, self.bias, 1e-5)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = FeedForward(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed, bias=config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):   \n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed % config.n_heads == 0, \"n_embed must be divisible by n_heads\"\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_embed = config.n_embed\n",
    "        self.dropout = config.dropout\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.context_length, config.context_length)).view(1, 1, config.context_length, config.context_length))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        \n",
    "        # compute attention scores\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # B, n_heads, T, T x B, n_heads, T, head_size  -> B, n_heads, T, head_size\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # B, T, n_heads * head_size\n",
    "        y = self.c_proj(y) # B, T, C\n",
    "        return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fce792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n",
      "total_batch_size:  256\n",
      "=> calculated grad_accumulation_steps:  1\n",
      "number of parameters: 91.27M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "User specified an unsupported autocast device_type 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_bf16_supported() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m     37\u001b[0m ptdtype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat32, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfloat16}[dtype]\n\u001b[0;32m---> 38\u001b[0m ctx \u001b[38;5;241m=\u001b[39m nullcontext() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_loss\u001b[39m():\n\u001b[1;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.9/site-packages/torch/amp/autocast_mode.py:241\u001b[0m, in \u001b[0;36mautocast.__init__\u001b[0;34m(self, device_type, dtype, enabled, cache_enabled)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_device_mod\u001b[38;5;241m.\u001b[39mget_autocast_dtype()\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser specified an unsupported autocast device_type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m     )\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    246\u001b[0m     enabled\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39mamp_definitely_not_available()\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: User specified an unsupported autocast device_type 'mps'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "total_batch_size, B, T = 128, 2, 64 # Total Batch Size, Micro Batch Size, Sequence Length\n",
    "assert total_batch_size % (B * T) == 0, f\"total_batch_size {total_batch_size} must be divisible by B*T {B*T}\"\n",
    "grad_accumulation_steps = total_batch_size // (B * T)\n",
    "print(\"total_batch_size: \", total_batch_size)\n",
    "print(\"=> calculated grad_accumulation_steps: \", grad_accumulation_steps)\n",
    "\n",
    "data_loader = DataLoader(B=2, T=256, dataset='shakespeare')\n",
    "train_loader = data_loader.get_batch('train')\n",
    "validation_loader = data_loader.get_batch('val')\n",
    "torch.set_float32_matmul_precision('medium') # set 'medium' for bfloat16, 'high' for tf32, 'highest' for float32\n",
    "    \n",
    "# loss to expect upon initialization is -log(1/vocab_size) = -log(1/488) = 6.2\n",
    "model = GPT(Config())\n",
    "#model.eval()\n",
    "model.to(device)\n",
    "if device not in ['mps', 'cpu']:\n",
    "    model = torch.compile(model) # optimises model for the cuda device\n",
    "\n",
    "learning_rate = 1e-3\n",
    "min_lr = learning_rate * 0.1\n",
    "warmup_steps = 100\n",
    "max_steps = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = 250\n",
    "log_interval = 10\n",
    "best_val_loss = 1e9\n",
    "grad_clip = 1.0\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device in ['cpu', 'mps'] else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data_loader.get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def lr_scheduler(it):\n",
    "    # linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return learning_rate * (it+1) / warmup_steps\n",
    "    # if it > lr_decay_iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# optimisation\n",
    "X, Y = data_loader.get_batch('train')\n",
    "t0 = time.time()\n",
    "decay_lr = False\n",
    "local_iter_num = 0\n",
    "iter_num = 0\n",
    "loss_accum = 0.0\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "while True:\n",
    "    # determine the learning rate\n",
    "    lr = lr_scheduler(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # update the parameters\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss:\n",
    "            print(\"Saving to checkpoint...\")\n",
    "\n",
    "    for micro_step in range(grad_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / grad_accumulation_steps\n",
    "        loss_accum += loss.detach()\n",
    "        X, Y = data_loader.get_batch('train')\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    # clips gradients to prevent exploding gradients caused by bad data or initialization\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize(device=device)\n",
    "    if device == 'mps':\n",
    "        torch.backends.mps.sync()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "\n",
    "            \n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * grad_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "    torch.cuda.empty_cache()\n",
    "    # termination conditions\n",
    "    if iter_num > max_steps:\n",
    "        break\n",
    "# TODO: \n",
    "# do the logging\n",
    "# save the model to huggingface\n",
    "# use eval datasetto evaluate model\n",
    "# synthesize more newspeak corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116087d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid token id: 351",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(x, max_new_tokens, temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m, in \u001b[0;36mRegexTokenizer.decode\u001b[0;34m(self, ids)\u001b[0m\n\u001b[1;32m     84\u001b[0m         part_bytes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_special_tokens[idx]\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid token id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m text_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(part_bytes)\n\u001b[1;32m     88\u001b[0m text \u001b[38;5;241m=\u001b[39m text_bytes\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid token id: 351"
     ]
    }
   ],
   "source": [
    "start_ids = data_loader.tokenizer.encode(\"Hi, Im a language model\")\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "num_samples = 5\n",
    "max_new_tokens = 256\n",
    "temperature = 1.0\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature)\n",
    "            print(data_loader.tokenizer.decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360d96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
