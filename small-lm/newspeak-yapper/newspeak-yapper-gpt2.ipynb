{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23338b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains the base Tokenizer class and a few common helper functions.\n",
    "The base class also contains the (common) save/load functionality.\n",
    "It would be possible to be a lot more strict about the interface and\n",
    "e.g. isolating all regex/pattern parts to the RegexTokenizer, but\n",
    "some concessions are made for simplicity.\n",
    "\"\"\"\n",
    "import unicodedata\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# a few helper functions useful for both BasicTokenizer and RegexTokenizer\n",
    "\n",
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# first two helper functions...\n",
    "def replace_control_characters(s: str) -> str:\n",
    "    # we don't want to print control characters\n",
    "    # which distort the output (e.g. \\n or much worse)\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
    "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# the base Tokenizer class\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char ï¿½.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0d58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal (byte-level) Byte Pair Encoding tokenizer.\n",
    "\n",
    "Algorithmically follows along the GPT tokenizer:\n",
    "https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "\n",
    "Unlike BasicTokenizer:\n",
    "- RegexTokenizer handles an optional regex splitting pattern.\n",
    "- RegexTokenizer handles optional special tokens.\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # split the text up into text chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunk_ids, stats)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "        if none_raise, then an error is raised if any special token is encountered in text\n",
    "        this is the default tiktoken behavior right now as well\n",
    "        any other behavior is either annoying, or a major footgun\n",
    "        \"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        # otherwise, we have to be careful with potential special tokens in text\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d214616e-9ad9-447b-8ec8-42f34706e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        vocab_size = 488\n",
    "        tokenizer = RegexTokenizer()\n",
    "        df = pd.read_csv(\"newspeak.csv\")\n",
    "        text = \"\".join([v for v in df[\"Newspeak Translation\"]])\n",
    "\n",
    "        if not os.path.exists(\"tok400.model\"):\n",
    "            tokenizer.train(text, vocab_size=vocab_size, verbose=True) # 9746 is the size of the vocabulary\n",
    "            tokenizer.save(\"tok400\") # writes tok32k.model and tok32k.vocab\n",
    "        \n",
    "        tokenizer.load(\"tok400.model\") # loads the model back from disk\n",
    "        self.tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        print(\"loaded {len(self.tokens)} tokens\")\n",
    "        print(\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        self.curent_idx = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        # example batch of 4 sequences of length 6. we'll use the first 24 tokens as input, \n",
    "        # and the last 24 tokens as labels. this is a common trick in language modeling, \n",
    "        # where we want to predict the next token in the sequence.\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.curent_idx:self.curent_idx + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.curent_idx += B * T\n",
    "        if self.curent_idx + B * T > len(self.tokens):\n",
    "            self.curent_idx = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 512\n",
    "    n_embed: int = 384\n",
    "    n_heads: int = 6\n",
    "    n_layers: int = 6\n",
    "    context_length: int = 256\n",
    "    dropout: float = 0.2\n",
    "    bias: bool = True \n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "            wpe = nn.Embedding(config.context_length, config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layers)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embed, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight # tie weights | weight sharing scheme\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                # this is the projection layer of the attention head\n",
    "                # we want to initialize it with the same weights as the embedding layer\n",
    "                # multiply n_layers by 2, because we have 2 additive connections in the block\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layers))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Returns the number of parameters in the model.\n",
    "        If non_embedding is True, it excludes the embedding layer.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name):\n",
    "        # load pretrained model from huggingface\n",
    "        assert model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        config_args = {\n",
    "            \"gpt2\": {\"n_embed\": 768, \"n_heads\": 12, \"n_layers\": 12},\n",
    "            \"gpt2-medium\": {\"n_embed\": 1024, \"n_heads\": 16, \"n_layers\": 24},\n",
    "            \"gpt2-large\": {\"n_embed\": 1280, \"n_heads\": 20, \"n_layers\": 36},\n",
    "            \"gpt2-xl\": {\"n_embed\": 1600, \"n_heads\": 25, \"n_layers\": 48},\n",
    "        }[model_name]\n",
    "        config_args[\"context_length\"] = 1024\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config = Config(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = [k for k in sd.keys() if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        # remove the extra keys from the huggingface model\n",
    "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith(\".attn.bias\") or not k.endswith(\".attn.masked_bias\")]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        for k in sd_keys_hf:\n",
    "            if k not in sd_keys:\n",
    "                print(f\"Warning: {k} not in model state dict\")\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"Mismatch in number of keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch for {k}: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "        \n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch for {k}: {sd_hf[k].shape} != {sd[k].shape}\"\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # B, T\n",
    "        # make sure the input is within the context length\n",
    "        assert T <= self.config.context_length, f\"Cannot forward, model block size is {self.config.context_length}, but input is {T}\"\n",
    "        # forward the token embeddings and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # positional embedding of shape (T, n_embed)\n",
    "        tok_emb = self.transformer.wte(idx) # token embedding of shape (B, T, n_embed)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layer norm and the lm head\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :]) # shape (B, 1, vocab_size)\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # create optim groups. any parameters that is 2D will be weight delayed, otherwise not\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay , all biases and layernorms dont\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"number of decay params: {num_decay_params / 1e6:.2f}M\")\n",
    "        print(f\"number of nodecay params: {num_nodecay_params / 1e6:.2f}M\")\n",
    "        # create the optimizer and use fused version if available \n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        print(\"using fused adamw\" if use_fused else \"using adamw\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm with optioanl bias\"\"\"\n",
    "\n",
    "    def __init__(self, n_dim, bias):\n",
    "        super().__init__()\n",
    "        self.weigth = nn.Parameter(torch.ones(n_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weigth.shape, self.weigth, self.bias, 1e-5)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = FeedForward(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed, bias=config.bias)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed, bias=config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):   \n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed % config.n_heads == 0, \"n_embed must be divisible by n_heads\"\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_embed = config.n_embed\n",
    "        self.dropout = config.dropout\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.context_length, config.context_length)).view(1, 1, config.context_length, config.context_length))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2) # B, n_heads, T, C // n_heads\n",
    "        \n",
    "        # compute attention scores\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # B, n_heads, T, T x B, n_heads, T, head_size  -> B, n_heads, T, head_size\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # B, T, n_heads * head_size\n",
    "        y = self.c_proj(y) # B, T, C\n",
    "        return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fce792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n",
      "total_batch_size:  524288\n",
      "=> calculated grad_accumulation_steps:  32\n",
      "loaded {len(self.tokens)} tokens\n",
      "1 epoch = {len(self.tokens) // (B * T)} batches\n",
      "number of parameters: 10.84M\n",
      "number of decay params: 10.91M\n",
      "number of nodecay params: 0.03M\n",
      "using adamw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | loss: 6.284475 | lr: 6.0000e-05 | norm: 1.0211e+01 | time: 1419.85ms | tokens/s: 2884.82\n",
      "step: 1 | loss: 5.584973 | lr: 1.2000e-04 | norm: 6.1113e+00 | time: 571.91ms | tokens/s: 7161.99\n",
      "step: 2 | loss: 5.116330 | lr: 1.8000e-04 | norm: 4.4456e+00 | time: 544.29ms | tokens/s: 7525.44\n",
      "step: 3 | loss: 4.736885 | lr: 2.4000e-04 | norm: 5.6074e+00 | time: 548.44ms | tokens/s: 7468.50\n",
      "step: 4 | loss: 4.274448 | lr: 3.0000e-04 | norm: 1.8683e+01 | time: 553.03ms | tokens/s: 7406.50\n",
      "step: 5 | loss: 3.855610 | lr: 3.6000e-04 | norm: 8.9580e+00 | time: 554.29ms | tokens/s: 7389.66\n",
      "step: 6 | loss: 3.420907 | lr: 4.2000e-04 | norm: 1.1704e+01 | time: 557.43ms | tokens/s: 7347.98\n",
      "step: 7 | loss: 3.091822 | lr: 4.8000e-04 | norm: 7.4358e+00 | time: 553.31ms | tokens/s: 7402.67\n",
      "step: 8 | loss: 2.812276 | lr: 5.4000e-04 | norm: 8.0808e+00 | time: 550.07ms | tokens/s: 7446.38\n",
      "step: 9 | loss: 2.536636 | lr: 6.0000e-04 | norm: 5.5387e+00 | time: 548.23ms | tokens/s: 7471.29\n",
      "step: 10 | loss: 2.289028 | lr: 6.0000e-04 | norm: 3.8207e+00 | time: 555.39ms | tokens/s: 7374.96\n",
      "step: 11 | loss: 2.118034 | lr: 5.9917e-04 | norm: 3.3760e+00 | time: 617.95ms | tokens/s: 6628.35\n",
      "step: 12 | loss: 1.940024 | lr: 5.9668e-04 | norm: 2.5178e+00 | time: 579.65ms | tokens/s: 7066.39\n",
      "step: 13 | loss: 1.824201 | lr: 5.9254e-04 | norm: 1.6810e+00 | time: 548.31ms | tokens/s: 7470.17\n",
      "step: 14 | loss: 1.719018 | lr: 5.8679e-04 | norm: 2.6447e+00 | time: 564.49ms | tokens/s: 7256.13\n",
      "step: 15 | loss: 1.643634 | lr: 5.7945e-04 | norm: 1.6428e+00 | time: 553.64ms | tokens/s: 7398.34\n",
      "step: 16 | loss: 1.556209 | lr: 5.7057e-04 | norm: 1.1608e+00 | time: 563.62ms | tokens/s: 7267.33\n",
      "step: 17 | loss: 1.473549 | lr: 5.6021e-04 | norm: 8.7789e-01 | time: 551.16ms | tokens/s: 7431.65\n",
      "step: 18 | loss: 1.421013 | lr: 5.4843e-04 | norm: 8.6562e-01 | time: 548.03ms | tokens/s: 7474.10\n",
      "step: 19 | loss: 1.346752 | lr: 5.3531e-04 | norm: 7.7430e-01 | time: 548.09ms | tokens/s: 7473.22\n",
      "step: 20 | loss: 1.311629 | lr: 5.2092e-04 | norm: 7.9004e-01 | time: 660.19ms | tokens/s: 6204.30\n",
      "step: 21 | loss: 1.249834 | lr: 5.0535e-04 | norm: 6.5555e-01 | time: 668.83ms | tokens/s: 6124.09\n",
      "step: 22 | loss: 1.227052 | lr: 4.8870e-04 | norm: 8.6350e-01 | time: 570.70ms | tokens/s: 7177.11\n",
      "step: 23 | loss: 1.191815 | lr: 4.7107e-04 | norm: 7.0696e-01 | time: 538.78ms | tokens/s: 7602.40\n",
      "step: 24 | loss: 1.162671 | lr: 4.5258e-04 | norm: 5.4380e-01 | time: 540.10ms | tokens/s: 7583.74\n",
      "step: 25 | loss: 1.139994 | lr: 4.3332e-04 | norm: 6.8422e-01 | time: 541.38ms | tokens/s: 7565.90\n",
      "step: 26 | loss: 1.117369 | lr: 4.1343e-04 | norm: 5.7688e-01 | time: 570.25ms | tokens/s: 7182.79\n",
      "step: 27 | loss: 1.100932 | lr: 3.9303e-04 | norm: 5.0082e-01 | time: 734.16ms | tokens/s: 5579.20\n",
      "step: 28 | loss: 1.085375 | lr: 3.7224e-04 | norm: 5.9367e-01 | time: 613.61ms | tokens/s: 6675.28\n",
      "step: 29 | loss: 1.071762 | lr: 3.5118e-04 | norm: 5.5202e-01 | time: 607.00ms | tokens/s: 6747.97\n",
      "step: 30 | loss: 1.050609 | lr: 3.3000e-04 | norm: 4.3300e-01 | time: 577.24ms | tokens/s: 7095.86\n",
      "step: 31 | loss: 1.044167 | lr: 3.0882e-04 | norm: 6.1533e-01 | time: 566.16ms | tokens/s: 7234.72\n",
      "step: 32 | loss: 1.032236 | lr: 2.8776e-04 | norm: 5.8791e-01 | time: 564.32ms | tokens/s: 7258.23\n",
      "step: 33 | loss: 1.022712 | lr: 2.6697e-04 | norm: 4.9926e-01 | time: 544.14ms | tokens/s: 7527.47\n",
      "step: 34 | loss: 1.008948 | lr: 2.4657e-04 | norm: 7.7680e-01 | time: 557.03ms | tokens/s: 7353.26\n",
      "step: 35 | loss: 1.005172 | lr: 2.2668e-04 | norm: 6.7922e-01 | time: 549.13ms | tokens/s: 7459.07\n",
      "step: 36 | loss: 0.985765 | lr: 2.0742e-04 | norm: 5.5032e-01 | time: 549.63ms | tokens/s: 7452.23\n",
      "step: 37 | loss: 0.983201 | lr: 1.8893e-04 | norm: 5.6512e-01 | time: 549.88ms | tokens/s: 7448.97\n",
      "step: 38 | loss: 0.972632 | lr: 1.7130e-04 | norm: 8.4231e-01 | time: 550.15ms | tokens/s: 7445.20\n",
      "step: 39 | loss: 0.963877 | lr: 1.5465e-04 | norm: 6.1169e-01 | time: 581.52ms | tokens/s: 7043.63\n",
      "step: 40 | loss: 0.958259 | lr: 1.3908e-04 | norm: 7.3378e-01 | time: 545.43ms | tokens/s: 7509.69\n",
      "step: 41 | loss: 0.950586 | lr: 1.2469e-04 | norm: 6.3594e-01 | time: 546.62ms | tokens/s: 7493.38\n",
      "step: 42 | loss: 0.948302 | lr: 1.1157e-04 | norm: 7.9605e-01 | time: 558.34ms | tokens/s: 7336.07\n",
      "step: 43 | loss: 0.930073 | lr: 9.9787e-05 | norm: 5.4625e-01 | time: 543.55ms | tokens/s: 7535.70\n",
      "step: 44 | loss: 0.937516 | lr: 8.9428e-05 | norm: 7.0451e-01 | time: 545.97ms | tokens/s: 7502.27\n",
      "step: 45 | loss: 0.924726 | lr: 8.0553e-05 | norm: 5.9038e-01 | time: 556.12ms | tokens/s: 7365.30\n",
      "step: 46 | loss: 0.920796 | lr: 7.3215e-05 | norm: 5.9210e-01 | time: 545.01ms | tokens/s: 7515.49\n",
      "step: 47 | loss: 0.911534 | lr: 6.7460e-05 | norm: 7.3387e-01 | time: 545.43ms | tokens/s: 7509.69\n",
      "step: 48 | loss: 0.915762 | lr: 6.3324e-05 | norm: 5.4929e-01 | time: 553.89ms | tokens/s: 7395.02\n",
      "step: 49 | loss: 0.907313 | lr: 6.0832e-05 | norm: 6.0843e-01 | time: 548.78ms | tokens/s: 7463.81\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuburtbalanon/anaconda3/envs/llama/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "total_batch_size, B, T = 524288, 16, 1024 # Total Batch Size, Micro Batch Size, Sequence Length\n",
    "assert total_batch_size % (B * T) == 0, f\"total_batch_size {total_batch_size} must be divisible by B*T {B*T}\"\n",
    "grad_accumulation_steps = total_batch_size // (B * T)\n",
    "print(\"total_batch_size: \", total_batch_size)\n",
    "print(\"=> calculated grad_accumulation_steps: \", grad_accumulation_steps)\n",
    "\n",
    "train_loader = DataLoader(B=4, T=32)\n",
    "torch.set_float32_matmul_precision('medium') # set 'medium' for bfloat16, 'high' for tf32, 'highest' for float32\n",
    "\n",
    "# loss to expect upon initialization is -log(1/vocab_size) = -log(1/488) = 6.2\n",
    "model = GPT(Config())\n",
    "#model.eval()\n",
    "model.to(device)\n",
    "if device not in ['mps', 'cpu']:\n",
    "    model = torch.compile(model) # optimises model for the cuda device\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "\n",
    "def lr_scheduler(it):\n",
    "    # linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # if it > lr_decay_iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "# optimisation\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accumulation_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if device in ['mps', 'cpu']:\n",
    "            # MPS does not support autocast yet\n",
    "            logits, loss = model(x, y)\n",
    "        else:\n",
    "            # autocast automatically handles the precision and the device\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        loss = loss / grad_accumulation_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    # clips gradients to prevent exploding gradients caused by bad data or initialization\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine the learning rate\n",
    "    lr = lr_scheduler(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # update the parameters\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    # torch.cuda.synchronize(device=device)\n",
    "    t1 = time.time()\n",
    "    tokens_processed = (train_loader.B * train_loader.T) * grad_accumulation_steps\n",
    "    tokens_per_second = tokens_processed / (t1 - t0)\n",
    "    print(f\"step: {step} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4e} | time: {(t1 - t0)*1000:.2f}ms | tokens/s: {tokens_per_second:.2f}\")\n",
    "\n",
    "# TODO: \n",
    "# choose a larger dataset\n",
    "# load the dataset from huggingface and preprocess it\n",
    "# split dataset with train/val/test and load with dataloader separately\n",
    "# implement validation step in train loop\n",
    "# do the logging\n",
    "# save the model to huggingface\n",
    "\n",
    "import sys; sys.exit()\n",
    "# inference\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        # get the last token\n",
    "        logits = logits[:, -1, :] # B, T, C -> B, C\n",
    "        # sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
    "        next_token = torch.multinomial(topk_probs, num_samples=1)\n",
    "        next_token = torch.gather(topk_indices, -1, next_token)\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "    \n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"> {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116087d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
